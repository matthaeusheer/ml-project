{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from pprint import pprint\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "try:\n",
    "    from context import ml_project\n",
    "except:\n",
    "    import ml_project\n",
    "from ml_project.io import DataHandler\n",
    "\n",
    "# To surpress sklearn warnings\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "_ = np.seterr(divide='ignore', invalid='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How-To"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Download the zip folder holding the data  \n",
    "2) Create a directory inside the PROJECT_ROOT_DIR/data and give it a suitable name DIR_NAME, e.g. \"task1b_data\"   \n",
    "3) Extract the files from the zip folder into <DIR_NAME>  \n",
    "4) Set the correct DIR_NAME in the following cell...  (no need for full absolute path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data is at /Users/sluck/eth/introML/data/task2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "DIR_NAME = os.getcwd()+'/data/task2'\n",
    "print(f\"Data is at {DIR_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data and aggregate feature matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_handler = DataHandler(DIR_NAME)\n",
    "train_data = data_handler.load_train_data()\n",
    "final_test_data = data_handler.load_test_data()  # only used for submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "HELD_OUT_TEST_SET_SIZE = 0.00001  # used for out of sample classifier performance evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(train_data.drop(['y'], axis=1), \n",
    "                                                    train_data['y'], \n",
    "                                                    test_size=HELD_OUT_TEST_SET_SIZE,\n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "SHOW_PLOT = False  # switch to True if you want to see the scatter plot\n",
    "if SHOW_PLOT:\n",
    "    _ = scatter_matrix(pd.concat([y_train, X_train], axis=1), alpha=0.3, figsize=(12, 12))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " We removed 1.7008504252126062% of the data we have now a total of 1965 samples\n636\nThe minimal amount of samples is 636\n636\n1272\n1908\nWe have now after balancing 1908 samples\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from pipelinehelper import PipelineHelper\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Some outlier removal, I think the course assignment makers srinkle some random noise here and there we dont need\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "\n",
    "# outlier detector, roughly 80% should remain for a good result\n",
    "outlier_detection = DBSCAN(\n",
    "  eps = 23.0,\n",
    "  metric=\"euclidean\",\n",
    "  min_samples = 50,\n",
    "  n_jobs = -1)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# Question> is data balanced?\n",
    "plt.hist(y_train)\n",
    "# almost a it seems\n",
    "\n",
    "\n",
    "clusters = outlier_detection.fit_predict(X_train)\n",
    "\n",
    "# Reshape Features\n",
    "new_train=X_train\n",
    "new_train['outlier']=clusters\n",
    "feature_mat_train_new=new_train[new_train.outlier==0]\n",
    "feature_mat_train_new=feature_mat_train_new.drop(['outlier'],axis=1)\n",
    "\n",
    "# Reshape Targets\n",
    "new_y=y_train.to_frame()\n",
    "new_y['outlier']=clusters\n",
    "y_train_new=new_y[new_y.outlier==0]\n",
    "y_train_new=y_train_new.drop(['outlier'],axis=1).values.reshape((-1,))\n",
    "print(f\" We removed {100*(X_train.shape[0]-feature_mat_train_new.shape[0])/X_train.shape[0]}% of the data we have now a total of {feature_mat_train_new.shape[0]} samples\")\n",
    "\n",
    "assert y_train_new.shape[0]==feature_mat_train_new.shape[0]\n",
    "\n",
    "# lets balance classes\n",
    "new_combined=feature_mat_train_new\n",
    "new_combined['y']=y_train_new\n",
    "print(new_combined[new_combined.y==1].shape[0])\n",
    "min_sample_size=min(new_combined[new_combined.y==1].shape[0],\n",
    "                    new_combined[new_combined.y==2].shape[0],\n",
    "                    new_combined[new_combined.y==0].shape[0])\n",
    "print(f\"The minimal amount of samples is {min_sample_size}\")\n",
    "balanced_X=pd.DataFrame()\n",
    "balanced_Y=pd.DataFrame()\n",
    "for i in [0,1,2]:\n",
    "    #removed diffrence\n",
    "    balanced_X=balanced_X.append(new_combined[new_combined.y==i][:min_sample_size])\n",
    "    print(balanced_X.shape[0])\n",
    "# fix indexes\n",
    "balanced_Y=balanced_X.y\n",
    "balanced_X=balanced_X.drop(['y'],axis=1)\n",
    "\n",
    "assert balanced_X.shape[0]==balanced_Y.shape[0]\n",
    "balanced_Y=balanced_Y.values.reshape((-1,))\n",
    "\n",
    "        \n",
    "\n",
    "print(f\"We have now after balancing {balanced_X.shape[0]} samples\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape outlier removed>(1965, 21), (1965,)\nshape normal>(1999, 21), (1999,)\nFitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:   12.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15.4 s, sys: 211 ms, total: 15.6 s\nWall time: 27.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "pipe = Pipeline([('std', StandardScaler()),\n",
    "                 ('pca', PCA()),\n",
    "                 ('regr', PipelineHelper([\n",
    "                      ('rf', RandomForestClassifier(random_state=42)),\n",
    "                  ])),\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    'pca__n_components': [16],\n",
    "    'regr__selected_model': pipe.named_steps['regr'].generate({\n",
    "        'rf__bootstrap': [False],\n",
    "        'rf__max_depth': [None],\n",
    "        'rf__max_features': ['auto'],\n",
    "        'rf__min_samples_leaf': [1],\n",
    "        'rf__min_samples_split': [10],\n",
    "        'rf__n_estimators': [2000] \n",
    "    })\n",
    "}\n",
    "\n",
    "grid_cv = RandomizedSearchCV(pipe, param_distributions=param_grid, n_iter=10, cv=5, verbose=True, \n",
    "                             refit=True, n_jobs=-1)\n",
    "\n",
    "outlier_remove=True\n",
    "print(f\"shape outlier removed>{feature_mat_train_new.shape}, {y_train_new.shape}\")\n",
    "print(f\"shape normal>{X_train.shape}, {y_train.shape}\")\n",
    "\n",
    "if outlier_remove==True:\n",
    "    grid_cv = grid_cv.fit(balanced_X, balanced_Y)\n",
    "else:\n",
    "    grid_cv = grid_cv.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best param set: \n{'pca__n_components': 16,\n 'regr__selected_model': ('rf',\n                          {'bootstrap': False,\n                           'max_depth': None,\n                           'max_features': 'auto',\n                           'min_samples_leaf': 1,\n                           'min_samples_split': 10,\n                           'n_estimators': 2000})}\n\n ----------\nMean test scores for parameter combinations...\n0.834 (+/- 0.051) for {'regr__selected_model': ('rf', {'bootstrap': False, 'max_depth': None, 'max_features': 'auto', 'min_samples_leaf': 1, 'min_samples_split': 10, 'n_estimators': 2000}), 'pca__n_components': 16}\n----------\n"
     ]
    }
   ],
   "source": [
    "from ml_project.train import gridcv\n",
    "gridcv.print_gridcv_report(grid_cv, neg_sqr_of_score=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance evaluation on held out test dat\n",
    "This is the section where we get a sense of how well our trained model is doing on the part of the training set we did not touch during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scikitplot.metrics import plot_confusion_matrix\n",
    "from scikitplot.metrics import plot_roc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    y_pred = grid_cv.predict(X_test)\n",
    "    y_pred_proba = grid_cv.predict_proba(X_test)\n",
    "except:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plot_confusion_matrix(y_test, y_pred, figsize=(12, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 2 is out of bounds for axis 1 with size 2",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-86-0e53ab4ff205>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplot_roc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred_proba\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/scikitplot/metrics.py\u001b[0m in \u001b[0;36mplot_roc\u001b[0;34m(y_true, y_probas, title, plot_micro, plot_macro, classes_to_plot, ax, figsize, cmap, title_fontsize, text_fontsize)\u001b[0m\n\u001b[1;32m    412\u001b[0m     \u001b[0mindices_to_plot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses_to_plot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_plot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices_to_plot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 414\u001b[0;31m         fpr_dict[i], tpr_dict[i], _ = roc_curve(y_true, probas[:, i],\n\u001b[0m\u001b[1;32m    415\u001b[0m                                                 pos_label=classes[i])\n\u001b[1;32m    416\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mto_plot\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 2 is out of bounds for axis 1 with size 2"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "_ = plot_roc(y_test, y_pred_proba, figsize=(8, 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform prediction on provided test data set\n",
    "Now we perform predictions on the provided, unlabelled data set for submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = grid_cv.predict(final_test_data)\n",
    "y_pred_ids = final_test_data.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Store the data\n",
    "Putting everything into the right format and storing the results in the working data directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_handler.store_results_task2(y_pred, y_pred_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-intro",
   "language": "python",
   "name": "ml-intro"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
